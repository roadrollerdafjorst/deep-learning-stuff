{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data, normalizing and flattening it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading and Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11385 files belonging to 5 classes.\n",
      "Found 3795 files belonging to 5 classes.\n",
      "Found 3795 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = image_dataset_from_directory(\n",
    "    'data/train/',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=1,\n",
    "    image_size=(28, 28),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    color_mode='grayscale',\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "val = image_dataset_from_directory(\n",
    "    'data/val/',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=1,\n",
    "    image_size=(28, 28),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    color_mode='grayscale',\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "test = image_dataset_from_directory(\n",
    "    'data/test/',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=1,\n",
    "    image_size=(28, 28),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    color_mode='grayscale',\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "def normalize(image,label):\n",
    "    image = tf.cast(image/255. ,tf.float32)\n",
    "    return image, label\n",
    "\n",
    "train = train.map(normalize)\n",
    "val = val.map(normalize)\n",
    "test = test.map(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flattening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing training tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dataset and reshape each image tensor\n",
    "image_tensors = []\n",
    "label_tensors = []\n",
    "for image, labels in train:\n",
    "    num_images = image.shape[0]\n",
    "    image_vectors = tf.reshape(image, [num_images, -1])\n",
    "    image_tensors.append(image_vectors)\n",
    "    label_tensors.append(labels)\n",
    "\n",
    "# Concatenate the image tensors into a single tensor\n",
    "train_vectors = [tf.concat(image_tensors, axis=0), tf.concat(label_tensors, axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing validation tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dataset and reshape each image tensor\n",
    "image_tensors = []\n",
    "label_tensors = []\n",
    "for image, labels in val:\n",
    "    num_images = image.shape[0]\n",
    "    image_vectors = tf.reshape(image, [num_images, -1])\n",
    "    image_tensors.append(image_vectors)\n",
    "    label_tensors.append(labels)\n",
    "\n",
    "# Concatenate the image tensors into a single tensor\n",
    "val_vectors = [tf.concat(image_tensors, axis=0), tf.concat(label_tensors, axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing testing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dataset and reshape each image tensor\n",
    "image_tensors = []\n",
    "label_tensors = []\n",
    "for image, labels in test:\n",
    "    num_images = image.shape[0]\n",
    "    image_vectors = tf.reshape(image, [num_images, -1])\n",
    "    image_tensors.append(image_vectors)\n",
    "    label_tensors.append(labels)\n",
    "\n",
    "# Concatenate the image tensors into a single tensor\n",
    "test_vectors = [tf.concat(image_tensors, axis=0), tf.concat(label_tensors, axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding compressed representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduced_representation(dim, vec):\n",
    "    models = os.listdir('./models/Q22')\n",
    "    model_path = str\n",
    "    for model in models:\n",
    "        if( (\"1_hidden\" in model) and ((str(dim)+\"_n\") in model)) : model_path = model\n",
    "    loaded_model = tf.keras.models.load_model(os.path.join('./models/Q22', model_path))\n",
    "    hidden_layer_model = tf.keras.models.Model(inputs=loaded_model.input, outputs=loaded_model.layers[1].output)\n",
    "    hidden_output = hidden_layer_model.predict(vec[0].numpy().reshape(len(vec[0]),784))\n",
    "    return hidden_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models with different architectures\n",
      "356/356 [==============================] - 1s 4ms/step\n",
      "119/119 [==============================] - 0s 3ms/step\n",
      "32-96-64-32...\n",
      "epochs: 375, acc: 0.9943785667419434\n",
      "\n",
      "32-64-96-128...\n",
      "epochs: 320, acc: 0.991304337978363\n",
      "\n",
      "32-128-96-64...\n",
      "epochs: 357, acc: 0.9949933886528015\n",
      "\n",
      "32-256-128-96...\n",
      "epochs: 284, acc: 0.9927975535392761\n",
      "\n",
      "356/356 [==============================] - 1s 3ms/step\n",
      "119/119 [==============================] - 0s 3ms/step\n",
      "64-96-64-32...\n",
      "epochs: 321, acc: 0.9993851780891418\n",
      "\n",
      "64-64-96-128...\n",
      "epochs: 229, acc: 0.9966622591018677\n",
      "\n",
      "64-128-96-64...\n",
      "epochs: 229, acc: 0.9986824989318848\n",
      "\n",
      "64-256-128-96...\n",
      "epochs: 177, acc: 0.9978919625282288\n",
      "\n",
      "356/356 [==============================] - 2s 4ms/step\n",
      "119/119 [==============================] - 0s 3ms/step\n",
      "128-96-64-32...\n",
      "epochs: 245, acc: 0.9992973208427429\n",
      "\n",
      "128-64-96-128...\n",
      "epochs: 171, acc: 0.9990338087081909\n",
      "\n",
      "128-128-96-64...\n",
      "epochs: 188, acc: 0.999472975730896\n",
      "\n",
      "128-256-128-96...\n",
      "epochs: 175, acc: 0.9995608329772949\n",
      "\n",
      "356/356 [==============================] - 2s 5ms/step\n",
      "119/119 [==============================] - 0s 3ms/step\n",
      "256-96-64-32...\n",
      "epochs: 226, acc: 0.9992973208427429\n",
      "\n",
      "256-64-96-128...\n",
      "epochs: 156, acc: 0.999736487865448\n",
      "\n",
      "256-128-96-64...\n",
      "epochs: 188, acc: 0.999736487865448\n",
      "\n",
      "256-256-128-96...\n",
      "epochs: 140, acc: 0.9999121427536011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_arch = [\n",
    "    [96, 64, 32],\n",
    "    [64, 96, 128],\n",
    "    [128, 96, 64],\n",
    "    [256, 128, 96]\n",
    "]\n",
    "initializer = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=42)\n",
    "\n",
    "k=5 # no. of classes\n",
    "model_history = dict()\n",
    "\n",
    "# train different achitectures and optimizers\n",
    "print('Training models with different architectures')\n",
    "for reduced_dimension in [32,64,128,256]:\n",
    "    reduced_rep_train = get_reduced_representation(reduced_dimension, train_vectors)\n",
    "    reduced_rep_val = get_reduced_representation(reduced_dimension, val_vectors)\n",
    "    for layer_dims in model_arch:\n",
    "        print(f'{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}...')\n",
    "        # define model\n",
    "        model = Sequential([\n",
    "            layers.Dense(reduced_dimension, activation=\"relu\", input_shape=(reduced_dimension,)),\n",
    "            # keras.Input(input_shape=(reduced_dimension,)),\n",
    "            layers.Dense(layer_dims[0], activation=\"tanh\", name=\"layer1\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            layers.Dense(layer_dims[1], activation=\"tanh\", name=\"layer2\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            layers.Dense(layer_dims[2], activation=\"tanh\", name=\"layer3\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            # layers.Dense(layer_dims[3], activation=\"sigmoid\", name=\"layer4\", \n",
    "            #              kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            layers.Dense(k, activation=\"softmax\", name=\"output\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "        ])\n",
    "        \n",
    "        # compile model\n",
    "        adam_optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "        model.compile(optimizer=adam_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # callbacks\n",
    "        my_callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5),\n",
    "            TensorBoard(log_dir=f'./logdir/Q33/{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}/')\n",
    "        ]\n",
    "        model_fit = model.fit(reduced_rep_train, train_vectors[1].numpy(), batch_size=len(train_vectors[0]), epochs=10000, verbose=0, callbacks=my_callbacks, \n",
    "                              validation_split=0.0, validation_data=(reduced_rep_val, val_vectors[1].numpy()), shuffle=True, validation_batch_size=None)\n",
    "        \n",
    "        model_history[f'{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}'] = model_fit.history['accuracy']\n",
    "        \n",
    "        hist_metric = 'accuracy'\n",
    "        print(f'epochs: {len(model_fit.history[hist_metric])}, acc: {model_fit.history[hist_metric][-1]}\\n')\n",
    "        model.save(f'models/Q33/{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}.tf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356/356 [==============================] - 1s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "32-96-64-32...\n",
      "356/356 [==============================] - 0s 691us/step - loss: 0.0327 - accuracy: 0.9945\n",
      "119/119 [==============================] - 0s 732us/step - loss: 0.0961 - accuracy: 0.9729\n",
      "119/119 [==============================] - 0s 710us/step - loss: 0.1031 - accuracy: 0.9731\n",
      "32-64-96-128...\n",
      "356/356 [==============================] - 0s 694us/step - loss: 0.0299 - accuracy: 0.9914\n",
      "119/119 [==============================] - 0s 727us/step - loss: 0.0760 - accuracy: 0.9779\n",
      "119/119 [==============================] - 0s 718us/step - loss: 0.0951 - accuracy: 0.9715\n",
      "32-128-96-64...\n",
      "356/356 [==============================] - 0s 694us/step - loss: 0.0234 - accuracy: 0.9952\n",
      "119/119 [==============================] - 0s 720us/step - loss: 0.0736 - accuracy: 0.9773\n",
      "119/119 [==============================] - 0s 778us/step - loss: 0.0901 - accuracy: 0.9744\n",
      "32-256-128-96...\n",
      "356/356 [==============================] - 0s 767us/step - loss: 0.0324 - accuracy: 0.9928\n",
      "119/119 [==============================] - 0s 786us/step - loss: 0.0883 - accuracy: 0.9731\n",
      "119/119 [==============================] - 0s 785us/step - loss: 0.0988 - accuracy: 0.9708\n",
      "356/356 [==============================] - 1s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "64-96-64-32...\n",
      "356/356 [==============================] - 0s 714us/step - loss: 0.0157 - accuracy: 0.9994\n",
      "119/119 [==============================] - 0s 725us/step - loss: 0.0791 - accuracy: 0.9792\n",
      "119/119 [==============================] - 0s 710us/step - loss: 0.0821 - accuracy: 0.9797\n",
      "64-64-96-128...\n",
      "356/356 [==============================] - 0s 705us/step - loss: 0.0184 - accuracy: 0.9969\n",
      "119/119 [==============================] - 0s 769us/step - loss: 0.0635 - accuracy: 0.9797\n",
      "119/119 [==============================] - 0s 718us/step - loss: 0.0699 - accuracy: 0.9781\n",
      "64-128-96-64...\n",
      "356/356 [==============================] - 0s 705us/step - loss: 0.0114 - accuracy: 0.9987\n",
      "119/119 [==============================] - 0s 845us/step - loss: 0.0656 - accuracy: 0.9797\n",
      "119/119 [==============================] - 0s 735us/step - loss: 0.0714 - accuracy: 0.9787\n",
      "64-256-128-96...\n",
      "356/356 [==============================] - 0s 834us/step - loss: 0.0170 - accuracy: 0.9979\n",
      "119/119 [==============================] - 0s 818us/step - loss: 0.0884 - accuracy: 0.9747\n",
      "119/119 [==============================] - 0s 796us/step - loss: 0.0832 - accuracy: 0.9784\n",
      "356/356 [==============================] - 1s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "128-96-64-32...\n",
      "356/356 [==============================] - 0s 697us/step - loss: 0.0226 - accuracy: 0.9993\n",
      "119/119 [==============================] - 0s 727us/step - loss: 0.0720 - accuracy: 0.9845\n",
      "119/119 [==============================] - 0s 727us/step - loss: 0.0786 - accuracy: 0.9805\n",
      "128-64-96-128...\n",
      "356/356 [==============================] - 0s 708us/step - loss: 0.0077 - accuracy: 0.9991\n",
      "119/119 [==============================] - 0s 745us/step - loss: 0.0548 - accuracy: 0.9839\n",
      "119/119 [==============================] - 0s 727us/step - loss: 0.0603 - accuracy: 0.9837\n",
      "128-128-96-64...\n",
      "356/356 [==============================] - 0s 711us/step - loss: 0.0097 - accuracy: 0.9995\n",
      "119/119 [==============================] - 0s 794us/step - loss: 0.0600 - accuracy: 0.9839\n",
      "119/119 [==============================] - 0s 815us/step - loss: 0.0630 - accuracy: 0.9821\n",
      "128-256-128-96...\n",
      "356/356 [==============================] - 0s 815us/step - loss: 0.0047 - accuracy: 0.9996\n",
      "119/119 [==============================] - 0s 845us/step - loss: 0.0573 - accuracy: 0.9842\n",
      "119/119 [==============================] - 0s 837us/step - loss: 0.0558 - accuracy: 0.9831\n",
      "356/356 [==============================] - 1s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "256-96-64-32...\n",
      "356/356 [==============================] - 0s 851us/step - loss: 0.0247 - accuracy: 0.9993\n",
      "119/119 [==============================] - 0s 820us/step - loss: 0.0714 - accuracy: 0.9850\n",
      "119/119 [==============================] - 0s 820us/step - loss: 0.0734 - accuracy: 0.9845\n",
      "256-64-96-128...\n",
      "356/356 [==============================] - 0s 792us/step - loss: 0.0048 - accuracy: 0.9997\n",
      "119/119 [==============================] - 0s 794us/step - loss: 0.0482 - accuracy: 0.9850\n",
      "119/119 [==============================] - 0s 786us/step - loss: 0.0520 - accuracy: 0.9852\n",
      "256-128-96-64...\n",
      "356/356 [==============================] - 0s 888us/step - loss: 0.0058 - accuracy: 0.9997\n",
      "119/119 [==============================] - 0s 921us/step - loss: 0.0506 - accuracy: 0.9860\n",
      "119/119 [==============================] - 0s 862us/step - loss: 0.0579 - accuracy: 0.9847\n",
      "256-256-128-96...\n",
      "356/356 [==============================] - 0s 958us/step - loss: 0.0040 - accuracy: 0.9999\n",
      "119/119 [==============================] - 0s 913us/step - loss: 0.0507 - accuracy: 0.9866\n",
      "119/119 [==============================] - 0s 921us/step - loss: 0.0589 - accuracy: 0.9842\n"
     ]
    }
   ],
   "source": [
    "model_arch = [\n",
    "    [96, 64, 32],\n",
    "    [64, 96, 128],\n",
    "    [128, 96, 64],\n",
    "    [256, 128, 96]\n",
    "]\n",
    "initializer = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=42)\n",
    "\n",
    "k=5 # no. of classes\n",
    "for reduced_dimension in [32,64,128,256]:\n",
    "    reduced_rep_train = get_reduced_representation(reduced_dimension, train_vectors)\n",
    "    reduced_rep_val = get_reduced_representation(reduced_dimension, val_vectors)\n",
    "    reduced_rep_test = get_reduced_representation(reduced_dimension, test_vectors)\n",
    "    for layer_dims in model_arch:\n",
    "        print(f'{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}...')\n",
    "        \n",
    "        model_path = f'models/Q33/{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}.tf'\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        model.evaluate(reduced_rep_train,train_vectors[1].numpy())\n",
    "        model.evaluate(reduced_rep_val,val_vectors[1].numpy())    \n",
    "        model.evaluate(reduced_rep_test,test_vectors[1].numpy())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 0s 4ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "confusion matrix (test):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "best_arch = '256-128-96'\n",
    "best_red = 32\n",
    "best_model = tf.keras.models.load_model(\n",
    "            f'models/Q33/{best_red}-{best_arch}.tf')\n",
    "\n",
    "test_pred = best_model.predict(get_reduced_representation(best_red, test_vectors))\n",
    "pred_class_test = np.argmax(test_pred, axis=1)\n",
    "\n",
    "test_score = accuracy_score(test_vectors[1].numpy(), pred_class_test)\n",
    "\n",
    "print('confusion matrix (test):')\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "#fig.suptitle('Confusion Matrix (Test Set)', y=0.04, fontsize=15)\n",
    "ax = fig.add_subplot(111)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(test_vectors[1].numpy(), pred_class_test), display_labels=['0', '1', '5', '7', '8'])\n",
    "cm_display.plot(ax = ax, cmap='Greys', colorbar=False)\n",
    "ax.set_title(f'arch: {best_arch}, acc: {np.round(test_score, 4)}', fontdict = {'fontsize':14}, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduced_representation(dim, vec):\n",
    "    models = os.listdir('./models/Q22')\n",
    "    model_path = str\n",
    "    for model in models:\n",
    "        if( (\"3_hidden\" in model) and ((str(dim)+\"_n\") in model)) : model_path = model\n",
    "    loaded_model = tf.keras.models.load_model(os.path.join('./models/Q22', model_path))\n",
    "    hidden_layer_model = tf.keras.models.Model(inputs=loaded_model.input, outputs=loaded_model.layers[2].output)\n",
    "    hidden_output = hidden_layer_model.predict(vec[0].numpy().reshape(len(vec[0]),784))\n",
    "    return hidden_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models with different architectures\n",
      "356/356 [==============================] - 1s 4ms/step\n",
      "119/119 [==============================] - 0s 3ms/step\n",
      "32-96-64-32...\n",
      "epochs: 432, acc: 0.9951691031455994\n",
      "\n",
      "32-64-96-128...\n",
      "epochs: 375, acc: 0.9952569007873535\n",
      "\n",
      "32-128-96-64...\n",
      "epochs: 321, acc: 0.991655707359314\n",
      "\n",
      "32-256-128-96...\n",
      "epochs: 306, acc: 0.9934123754501343\n",
      "\n",
      "356/356 [==============================] - 2s 5ms/step\n",
      "119/119 [==============================] - 1s 4ms/step\n",
      "64-96-64-32...\n",
      "epochs: 367, acc: 0.9995608329772949\n",
      "\n",
      "64-64-96-128...\n",
      "epochs: 265, acc: 0.9989460110664368\n",
      "\n",
      "64-128-96-64...\n",
      "epochs: 313, acc: 0.9995608329772949\n",
      "\n",
      "64-256-128-96...\n",
      "epochs: 207, acc: 0.9980676174163818\n",
      "\n",
      "356/356 [==============================] - 3s 7ms/step\n",
      "119/119 [==============================] - 0s 4ms/step\n",
      "128-96-64-32...\n",
      "epochs: 256, acc: 0.9992973208427429\n",
      "\n",
      "128-64-96-128...\n",
      "epochs: 179, acc: 0.999472975730896\n",
      "\n",
      "128-128-96-64...\n",
      "epochs: 254, acc: 0.9998243451118469\n",
      "\n",
      "128-256-128-96...\n",
      "epochs: 178, acc: 0.9998243451118469\n",
      "\n",
      "356/356 [==============================] - 2s 4ms/step\n",
      "119/119 [==============================] - 1s 4ms/step\n",
      "256-96-64-32...\n",
      "epochs: 266, acc: 0.9995608329772949\n",
      "\n",
      "256-64-96-128...\n",
      "epochs: 155, acc: 0.9996486902236938\n",
      "\n",
      "256-128-96-64...\n",
      "epochs: 189, acc: 0.9998243451118469\n",
      "\n",
      "256-256-128-96...\n",
      "epochs: 165, acc: 0.9999121427536011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_arch = [\n",
    "    [96, 64, 32],\n",
    "    [64, 96, 128],\n",
    "    [128, 96, 64],\n",
    "    [256, 128, 96]\n",
    "]\n",
    "initializer = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=42)\n",
    "\n",
    "k=5 # no. of classes\n",
    "model_history = dict()\n",
    "\n",
    "# train different achitectures and optimizers\n",
    "print('Training models with different architectures')\n",
    "for reduced_dimension in [32,64,128,256]:\n",
    "    reduced_rep_train = get_reduced_representation(reduced_dimension, train_vectors)\n",
    "    reduced_rep_val = get_reduced_representation(reduced_dimension, val_vectors)\n",
    "    for layer_dims in model_arch:\n",
    "        print(f'{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}...')\n",
    "        # define model\n",
    "        model = Sequential([\n",
    "            layers.Dense(reduced_dimension, activation=\"relu\", input_shape=(reduced_dimension,)),\n",
    "            # keras.Input(input_shape=(reduced_dimension,)),\n",
    "            layers.Dense(layer_dims[0], activation=\"tanh\", name=\"layer1\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            layers.Dense(layer_dims[1], activation=\"tanh\", name=\"layer2\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            layers.Dense(layer_dims[2], activation=\"tanh\", name=\"layer3\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            # layers.Dense(layer_dims[3], activation=\"sigmoid\", name=\"layer4\", \n",
    "            #              kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            layers.Dense(k, activation=\"softmax\", name=\"output\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "        ])\n",
    "        \n",
    "        # compile model\n",
    "        adam_optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "        model.compile(optimizer=adam_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # callbacks\n",
    "        my_callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', min_delta=1e-7, patience=10),\n",
    "            TensorBoard(log_dir=f'./logdir/Q44/{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}/')\n",
    "        ]\n",
    "        model_fit = model.fit(reduced_rep_train, train_vectors[1].numpy(), batch_size=len(train_vectors[0]), epochs=10000, verbose=0, callbacks=my_callbacks, \n",
    "                              validation_split=0.0, validation_data=(reduced_rep_val, val_vectors[1].numpy()), shuffle=True, validation_batch_size=None)\n",
    "        \n",
    "        model_history[f'{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}'] = model_fit.history['accuracy']\n",
    "        \n",
    "        hist_metric = 'accuracy'\n",
    "        print(f'epochs: {len(model_fit.history[hist_metric])}, acc: {model_fit.history[hist_metric][-1]}\\n')\n",
    "        model.save(f'models/Q44/{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356/356 [==============================] - 2s 6ms/step\n",
      "119/119 [==============================] - 1s 4ms/step\n",
      "119/119 [==============================] - 0s 4ms/step\n",
      "32-96-64-32...\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.0270 - accuracy: 0.9952\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 0.0768 - accuracy: 0.9763\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0763 - accuracy: 0.9781\n",
      "32-64-96-128...\n",
      "356/356 [==============================] - 1s 3ms/step - loss: 0.0207 - accuracy: 0.9953\n",
      "119/119 [==============================] - 1s 4ms/step - loss: 0.0655 - accuracy: 0.9823\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0923 - accuracy: 0.9752\n",
      "32-128-96-64...\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.0309 - accuracy: 0.9917\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 0.0785 - accuracy: 0.9752\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 0.0970 - accuracy: 0.9736\n",
      "32-256-128-96...\n",
      "356/356 [==============================] - 1s 3ms/step - loss: 0.0235 - accuracy: 0.9935\n",
      "119/119 [==============================] - 0s 4ms/step - loss: 0.0825 - accuracy: 0.9771\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0906 - accuracy: 0.9752\n",
      "356/356 [==============================] - 1s 4ms/step\n",
      "119/119 [==============================] - 0s 4ms/step\n",
      "119/119 [==============================] - 0s 4ms/step\n",
      "64-96-64-32...\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.0132 - accuracy: 0.9996\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.9837\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0787 - accuracy: 0.9808\n",
      "64-64-96-128...\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.0072 - accuracy: 0.9989\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0785 - accuracy: 0.9779\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0725 - accuracy: 0.9792\n",
      "64-128-96-64...\n",
      "356/356 [==============================] - 1s 3ms/step - loss: 0.0059 - accuracy: 0.9996\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0619 - accuracy: 0.9818\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.9789\n",
      "64-256-128-96...\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.0131 - accuracy: 0.9981\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 0.0708 - accuracy: 0.9784\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0765 - accuracy: 0.9779\n",
      "356/356 [==============================] - 1s 4ms/step\n",
      "119/119 [==============================] - 0s 4ms/step\n",
      "119/119 [==============================] - 0s 4ms/step\n",
      "128-96-64-32...\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.0248 - accuracy: 0.9993\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0711 - accuracy: 0.9858\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9829\n",
      "128-64-96-128...\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9995\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0516 - accuracy: 0.9837\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0566 - accuracy: 0.9834\n",
      "128-128-96-64...\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9998\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0464 - accuracy: 0.9863\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0568 - accuracy: 0.9860\n",
      "128-256-128-96...\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9998\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9837\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0538 - accuracy: 0.9850\n",
      "356/356 [==============================] - 2s 5ms/step\n",
      "119/119 [==============================] - 1s 4ms/step\n",
      "119/119 [==============================] - 1s 4ms/step\n",
      "256-96-64-32...\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.0211 - accuracy: 0.9996\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0732 - accuracy: 0.9834\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9842\n",
      "256-64-96-128...\n",
      "356/356 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9996\n",
      "119/119 [==============================] - 1s 4ms/step - loss: 0.0531 - accuracy: 0.9839\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9858\n",
      "256-128-96-64...\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9998\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9871\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0611 - accuracy: 0.9852\n",
      "256-256-128-96...\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.0028 - accuracy: 0.9999\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9874\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9850\n"
     ]
    }
   ],
   "source": [
    "model_arch = [\n",
    "    [96, 64, 32],\n",
    "    [64, 96, 128],\n",
    "    [128, 96, 64],\n",
    "    [256, 128, 96]\n",
    "]\n",
    "initializer = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=42)\n",
    "\n",
    "k=5 # no. of classes\n",
    "for reduced_dimension in [32,64,128,256]:\n",
    "    reduced_rep_train = get_reduced_representation(reduced_dimension, train_vectors)\n",
    "    reduced_rep_val = get_reduced_representation(reduced_dimension, val_vectors)\n",
    "    reduced_rep_test = get_reduced_representation(reduced_dimension, test_vectors)\n",
    "    for layer_dims in model_arch:\n",
    "        print(f'{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}...')\n",
    "        \n",
    "        model_path = f'models/Q44/{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}.tf'\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        model.evaluate(reduced_rep_train,train_vectors[1].numpy())\n",
    "        model.evaluate(reduced_rep_val,val_vectors[1].numpy())    \n",
    "        model.evaluate(reduced_rep_test,test_vectors[1].numpy())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data, normalizing and flattening it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading and Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11385 files belonging to 5 classes.\n",
      "Found 3795 files belonging to 5 classes.\n",
      "Found 3795 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = image_dataset_from_directory(\n",
    "    'Group_24/train/',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=1,\n",
    "    image_size=(28, 28),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    color_mode='grayscale',\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "val = image_dataset_from_directory(\n",
    "    'Group_24/val/',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=1,\n",
    "    image_size=(28, 28),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    color_mode='grayscale',\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "test = image_dataset_from_directory(\n",
    "    'Group_24/test/',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=1,\n",
    "    image_size=(28, 28),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    color_mode='grayscale',\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "def normalize(image,label):\n",
    "    image = tf.cast(image/255. ,tf.float32)\n",
    "    return image, label\n",
    "\n",
    "train = train.map(normalize)\n",
    "val = val.map(normalize)\n",
    "test = test.map(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flattening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing training tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dataset and reshape each image tensor\n",
    "image_tensors = []\n",
    "label_tensors = []\n",
    "for image, labels in train:\n",
    "    num_images = image.shape[0]\n",
    "    image_vectors = tf.reshape(image, [num_images, -1])\n",
    "    image_tensors.append(image_vectors)\n",
    "    label_tensors.append(labels)\n",
    "\n",
    "# Concatenate the image tensors into a single tensor\n",
    "train_vectors = [tf.concat(image_tensors, axis=0), tf.concat(label_tensors, axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing validation tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dataset and reshape each image tensor\n",
    "image_tensors = []\n",
    "label_tensors = []\n",
    "for image, labels in val:\n",
    "    num_images = image.shape[0]\n",
    "    image_vectors = tf.reshape(image, [num_images, -1])\n",
    "    image_tensors.append(image_vectors)\n",
    "    label_tensors.append(labels)\n",
    "\n",
    "# Concatenate the image tensors into a single tensor\n",
    "val_vectors = [tf.concat(image_tensors, axis=0), tf.concat(label_tensors, axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing testing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dataset and reshape each image tensor\n",
    "image_tensors = []\n",
    "label_tensors = []\n",
    "for image, labels in test:\n",
    "    num_images = image.shape[0]\n",
    "    image_vectors = tf.reshape(image, [num_images, -1])\n",
    "    image_tensors.append(image_vectors)\n",
    "    label_tensors.append(labels)\n",
    "\n",
    "# Concatenate the image tensors into a single tensor\n",
    "test_vectors = [tf.concat(image_tensors, axis=0), tf.concat(label_tensors, axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding compressed representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduced_representation(dim, vec):\n",
    "    models = os.listdir('./models/Q22')\n",
    "    model_path = str\n",
    "    for model in models:\n",
    "        if( (\"3_hidden\" in model) and ((str(dim)+\"_n\") in model)) : model_path = model\n",
    "    loaded_model = tf.keras.models.load_model(os.path.join('./models/Q22', model_path))\n",
    "    hidden_layer_model = tf.keras.models.Model(inputs=loaded_model.input, outputs=loaded_model.layers[2].output)\n",
    "    hidden_output = hidden_layer_model.predict(vec[0].numpy().reshape(len(vec[0]),784))\n",
    "    return hidden_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models with different architectures\n",
      "356/356 [==============================] - 1s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "32-96-64-32...\n",
      "epochs: 1073, acc: 0.9688186049461365\n",
      "\n",
      "32-64-96-128...\n",
      "epochs: 1161, acc: 0.9707509875297546\n",
      "\n",
      "32-128-96-64...\n",
      "epochs: 789, acc: 0.9662714004516602\n",
      "\n",
      "32-256-128-96...\n",
      "epochs: 715, acc: 0.9625823497772217\n",
      "\n",
      "356/356 [==============================] - 1s 3ms/step\n",
      "119/119 [==============================] - 0s 3ms/step\n",
      "64-96-64-32...\n",
      "epochs: 860, acc: 0.9942907094955444\n",
      "\n",
      "64-64-96-128...\n",
      "epochs: 680, acc: 0.9894598126411438\n",
      "\n",
      "64-128-96-64...\n",
      "epochs: 650, acc: 0.9899868369102478\n",
      "\n",
      "64-256-128-96...\n",
      "epochs: 509, acc: 0.9889327883720398\n",
      "\n",
      "356/356 [==============================] - 1s 3ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "128-96-64-32...\n",
      "epochs: 736, acc: 0.9980676174163818\n",
      "\n",
      "128-64-96-128...\n",
      "epochs: 459, acc: 0.9932367205619812\n",
      "\n",
      "128-128-96-64...\n",
      "epochs: 477, acc: 0.9968379735946655\n",
      "\n",
      "128-256-128-96...\n",
      "epochs: 418, acc: 0.9960474371910095\n",
      "\n",
      "356/356 [==============================] - 1s 3ms/step\n",
      "119/119 [==============================] - 0s 3ms/step\n",
      "256-96-64-32...\n",
      "epochs: 640, acc: 0.9983311295509338\n",
      "\n",
      "256-64-96-128...\n",
      "epochs: 351, acc: 0.9968379735946655\n",
      "\n",
      "256-128-96-64...\n",
      "epochs: 449, acc: 0.9979798197746277\n",
      "\n",
      "256-256-128-96...\n",
      "epochs: 364, acc: 0.9983311295509338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_arch = [\n",
    "    [96, 64, 32],\n",
    "    [64, 96, 128],\n",
    "    [128, 96, 64],\n",
    "    [256, 128, 96]\n",
    "]\n",
    "initializer = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=42)\n",
    "\n",
    "k=5 # no. of classes\n",
    "model_history = dict()\n",
    "\n",
    "# train different achitectures and optimizers\n",
    "print('Training models with different architectures')\n",
    "for reduced_dimension in [32,64,128,256]:\n",
    "    reduced_rep_train = get_reduced_representation(reduced_dimension, train_vectors)\n",
    "    reduced_rep_val = get_reduced_representation(reduced_dimension, val_vectors)\n",
    "    for layer_dims in model_arch:\n",
    "        print(f'{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}...')\n",
    "        # define model\n",
    "        model = Sequential([\n",
    "            layers.Dense(reduced_dimension, activation=\"relu\", input_shape=(reduced_dimension,)),\n",
    "            # keras.Input(input_shape=(reduced_dimension,)),\n",
    "            layers.Dense(layer_dims[0], activation=\"sigmoid\", name=\"layer1\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            layers.Dense(layer_dims[1], activation=\"sigmoid\", name=\"layer2\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            layers.Dense(layer_dims[2], activation=\"sigmoid\", name=\"layer3\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            # layers.Dense(layer_dims[3], activation=\"sigmoid\", name=\"layer4\", \n",
    "            #              kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            layers.Dense(k, activation=\"softmax\", name=\"output\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "        ])\n",
    "        \n",
    "        # compile model\n",
    "        adam_optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "        model.compile(optimizer=adam_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # callbacks\n",
    "        my_callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', min_delta=1e-7, patience=10),\n",
    "            TensorBoard(log_dir=f'./logdir/Q4/{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}/')\n",
    "        ]\n",
    "        model_fit = model.fit(reduced_rep_train, train_vectors[1].numpy(), batch_size=len(train_vectors[0]), epochs=10000, verbose=0, callbacks=my_callbacks, \n",
    "                              validation_split=0.0, validation_data=(reduced_rep_val, val_vectors[1].numpy()), shuffle=True, validation_batch_size=None)\n",
    "        \n",
    "        model_history[f'{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}'] = model_fit.history['accuracy']\n",
    "        \n",
    "        hist_metric = 'accuracy'\n",
    "        print(f'epochs: {len(model_fit.history[hist_metric])}, acc: {model_fit.history[hist_metric][-1]}\\n')\n",
    "        model.save(f'models/Q4/{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}.tf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356/356 [==============================] - 1s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "32-96-64-32...\n",
      "356/356 [==============================] - 0s 705us/step - loss: 0.0270 - accuracy: 0.9952\n",
      "119/119 [==============================] - 0s 761us/step - loss: 0.0768 - accuracy: 0.9763\n",
      "119/119 [==============================] - 0s 727us/step - loss: 0.0763 - accuracy: 0.9781\n",
      "32-64-96-128...\n",
      "356/356 [==============================] - 0s 697us/step - loss: 0.0207 - accuracy: 0.9953\n",
      "119/119 [==============================] - 0s 710us/step - loss: 0.0655 - accuracy: 0.9823\n",
      "119/119 [==============================] - 0s 710us/step - loss: 0.0923 - accuracy: 0.9752\n",
      "32-128-96-64...\n",
      "356/356 [==============================] - 0s 700us/step - loss: 0.0309 - accuracy: 0.9917\n",
      "119/119 [==============================] - 0s 719us/step - loss: 0.0785 - accuracy: 0.9752\n",
      "119/119 [==============================] - 0s 744us/step - loss: 0.0970 - accuracy: 0.9736\n",
      "32-256-128-96...\n",
      "356/356 [==============================] - 0s 774us/step - loss: 0.0235 - accuracy: 0.9935\n",
      "119/119 [==============================] - 0s 826us/step - loss: 0.0825 - accuracy: 0.9771\n",
      "119/119 [==============================] - 0s 791us/step - loss: 0.0906 - accuracy: 0.9752\n",
      "356/356 [==============================] - 1s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "64-96-64-32...\n",
      "356/356 [==============================] - 0s 691us/step - loss: 0.0132 - accuracy: 0.9996\n",
      "119/119 [==============================] - 0s 804us/step - loss: 0.0658 - accuracy: 0.9837\n",
      "119/119 [==============================] - 0s 727us/step - loss: 0.0787 - accuracy: 0.9808\n",
      "64-64-96-128...\n",
      "356/356 [==============================] - 0s 710us/step - loss: 0.0072 - accuracy: 0.9989\n",
      "119/119 [==============================] - 0s 744us/step - loss: 0.0785 - accuracy: 0.9779\n",
      "119/119 [==============================] - 0s 736us/step - loss: 0.0725 - accuracy: 0.9792\n",
      "64-128-96-64...\n",
      "356/356 [==============================] - 0s 714us/step - loss: 0.0059 - accuracy: 0.9996\n",
      "119/119 [==============================] - 0s 744us/step - loss: 0.0619 - accuracy: 0.9818\n",
      "119/119 [==============================] - 0s 752us/step - loss: 0.0745 - accuracy: 0.9789\n",
      "64-256-128-96...\n",
      "356/356 [==============================] - 0s 879us/step - loss: 0.0131 - accuracy: 0.9981\n",
      "119/119 [==============================] - 0s 854us/step - loss: 0.0708 - accuracy: 0.9784\n",
      "119/119 [==============================] - 0s 938us/step - loss: 0.0765 - accuracy: 0.9779\n",
      "356/356 [==============================] - 1s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "128-96-64-32...\n",
      "356/356 [==============================] - 1s 1ms/step - loss: 0.0248 - accuracy: 0.9993\n",
      "119/119 [==============================] - 0s 769us/step - loss: 0.0711 - accuracy: 0.9858\n",
      "119/119 [==============================] - 0s 735us/step - loss: 0.0781 - accuracy: 0.9829\n",
      "128-64-96-128...\n",
      "356/356 [==============================] - 0s 742us/step - loss: 0.0060 - accuracy: 0.9995\n",
      "119/119 [==============================] - 0s 837us/step - loss: 0.0516 - accuracy: 0.9837\n",
      "119/119 [==============================] - 0s 837us/step - loss: 0.0566 - accuracy: 0.9834\n",
      "128-128-96-64...\n",
      "356/356 [==============================] - 0s 742us/step - loss: 0.0046 - accuracy: 0.9998\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 0.0464 - accuracy: 0.9863\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.9860\n",
      "128-256-128-96...\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9998\n",
      "119/119 [==============================] - 0s 1ms/step - loss: 0.0527 - accuracy: 0.9837\n",
      "119/119 [==============================] - 0s 997us/step - loss: 0.0538 - accuracy: 0.9850\n",
      "356/356 [==============================] - 1s 3ms/step\n",
      "119/119 [==============================] - 0s 3ms/step\n",
      "119/119 [==============================] - 0s 2ms/step\n",
      "256-96-64-32...\n",
      "356/356 [==============================] - 0s 862us/step - loss: 0.0211 - accuracy: 0.9996\n",
      "119/119 [==============================] - 0s 854us/step - loss: 0.0732 - accuracy: 0.9834\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9842\n",
      "256-64-96-128...\n",
      "356/356 [==============================] - 0s 961us/step - loss: 0.0042 - accuracy: 0.9996\n",
      "119/119 [==============================] - 0s 820us/step - loss: 0.0531 - accuracy: 0.9839\n",
      "119/119 [==============================] - 0s 828us/step - loss: 0.0599 - accuracy: 0.9858\n",
      "256-128-96-64...\n",
      "356/356 [==============================] - 0s 921us/step - loss: 0.0054 - accuracy: 0.9998\n",
      "119/119 [==============================] - 0s 870us/step - loss: 0.0511 - accuracy: 0.9871\n",
      "119/119 [==============================] - 0s 955us/step - loss: 0.0611 - accuracy: 0.9852\n",
      "256-256-128-96...\n",
      "356/356 [==============================] - 1s 1ms/step - loss: 0.0028 - accuracy: 0.9999\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.0486 - accuracy: 0.9874\n",
      "119/119 [==============================] - 0s 947us/step - loss: 0.0581 - accuracy: 0.9850\n"
     ]
    }
   ],
   "source": [
    "model_arch = [\n",
    "    [96, 64, 32],\n",
    "    [64, 96, 128],\n",
    "    [128, 96, 64],\n",
    "    [256, 128, 96]\n",
    "]\n",
    "initializer = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=42)\n",
    "\n",
    "k=5 # no. of classes\n",
    "for reduced_dimension in [32,64,128,256]:\n",
    "    reduced_rep_train = get_reduced_representation(reduced_dimension, train_vectors)\n",
    "    reduced_rep_val = get_reduced_representation(reduced_dimension, val_vectors)\n",
    "    reduced_rep_test = get_reduced_representation(reduced_dimension, test_vectors)\n",
    "    for layer_dims in model_arch:\n",
    "        print(f'{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}...')\n",
    "        \n",
    "        model_path = f'models/Q44/{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}.tf'\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        model.evaluate(reduced_rep_train,train_vectors[1].numpy())\n",
    "        model.evaluate(reduced_rep_val,val_vectors[1].numpy())    \n",
    "        model.evaluate(reduced_rep_test,test_vectors[1].numpy())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 0s 2ms/step\n",
      "119/119 [==============================] - 0s 862us/step\n",
      "confusion matrix (test):\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "best_arch = '64-96-128'\n",
    "best_red = 32\n",
    "best_model = tf.keras.models.load_model(\n",
    "            f'models/Q44/{best_red}-{best_arch}.tf')\n",
    "\n",
    "test_pred = best_model.predict(get_reduced_representation(best_red, test_vectors))\n",
    "pred_class_test = np.argmax(test_pred, axis=1)\n",
    "\n",
    "test_score = accuracy_score(test_vectors[1].numpy(), pred_class_test)\n",
    "\n",
    "print('confusion matrix (test):')\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "#fig.suptitle('Confusion Matrix (Test Set)', y=0.04, fontsize=15)\n",
    "ax = fig.add_subplot(111)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(test_vectors[1].numpy(), pred_class_test), display_labels=['0', '1', '5', '7', '8'])\n",
    "cm_display.plot(ax = ax, cmap='Greys', colorbar=False)\n",
    "ax.set_title(f'arch: {best_arch}, acc: {np.round(test_score, 4)}', fontdict = {'fontsize':14}, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
